from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime, timedelta
from azure.storage.filedatalake import DataLakeServiceClient
import json
import logging

# Replace with your actual Azure Data Lake connection string
AZURE_CONNECTION_STRING = "YOUR_CONNECTION_STRING"
RETRY_TRACKER_VAR = "retry_config_tracker"
SOURCE_DAG_ID = "dag_a_id"
SOURCE_TASK_ID = "push_task"
MAX_RETRIES = 20

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

def check_file_exists(service_client, file_path):
    try:
        fs_name, *path_parts = file_path.strip("/").split("/")
        file_system = service_client.get_file_system_client(fs_name)
        file_client = file_system.get_file_client("/".join(path_parts))
        file_client.get_file_properties()
        return True
    except Exception as e:
        logging.info(f"File not found or error checking {file_path}: {e}")
        return False

def process_failed_configs(**context):
    ti = context['ti']

    # Pull failed configs from other DAG
    failed_configs = ti.xcom_pull(
        dag_id=SOURCE_DAG_ID,
        task_ids=SOURCE_TASK_ID,
        key='failed_configs'
    ) or []

    # Load current tracker
    tracker_json = Variable.get(RETRY_TRACKER_VAR, default_var='[]')
    retry_tracker = json.loads(tracker_json)

    # Avoid duplicates (name, rundate)
    existing_keys = {(cfg['name'], cfg['rundate']) for cfg in retry_tracker}
    for cfg in failed_configs:
        key = (cfg['name'], cfg['rundate'])
        if key not in existing_keys:
            retry_tracker.append(cfg)

    # Init Azure Data Lake client
    service_client = DataLakeServiceClient.from_connection_string(AZURE_CONNECTION_STRING)

    # Process tracker
    updated_tracker = []
    for cfg in retry_tracker:
        name = cfg['name']
        rundate = cfg['rundate']
        retry_count = cfg.get('retry_count', 0)

        file1_exists = check_file_exists(service_client, cfg['file_path_1'])
        file2_exists = check_file_exists(service_client, cfg['file_path_2'])

        if file1_exists and file2_exists:
            logging.info(f"Files found for {name} ({rundate}) — removing from retry tracker.")
            continue

        retry_count += 1
        if retry_count > MAX_RETRIES:
            logging.warning(f"Exceeded max retries for {name} ({rundate}) — removing.")
            continue

        cfg['retry_count'] = retry_count
        updated_tracker.append(cfg)

    # Save final tracker
    Variable.set(RETRY_TRACKER_VAR, json.dumps(updated_tracker))
    logging.info(f"Updated retry_config_tracker with {len(updated_tracker)} entries.")

# Define the DAG
with DAG(
    dag_id="retry_merge_dag",
    default_args=default_args,
    schedule_interval="@hourly",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["retry", "xcom", "azure"]
) as dag:

    merge_and_check = PythonOperator(
        task_id="merge_failed_configs_and_check_files",
        python_callable=process_failed_configs,
        provide_context=True,
    )
