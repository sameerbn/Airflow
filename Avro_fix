import io
from azure.storage.filedatalake import DataLakeFileClient
from pyarrow.parquet import ParquetFile
from fastavro import reader as avro_reader

def has_records_adls_file(file_client: DataLakeFileClient, max_bytes=1 * 1024 * 1024) -> bool:
    try:
        props = file_client.get_file_properties()
        file_size = props.size

        if file_size == 0:
            print(f"⚠️ File is empty: {file_client.path_name}")
            return False

        # Read first 1MB of the file
        read_size = min(file_size, max_bytes)
        file_data = file_client.download_file(offset=0, length=read_size).readall()

        # Check Avro
        if file_client.path_name.endswith(".avro"):
            try:
                with io.BytesIO(file_data) as f:
                    avro_r = avro_reader(f)
                    for _ in avro_r:
                        return True
                return False
            except Exception as e:
                print(f"❌ Avro read failed: {e}")
                return False

        # Check Parquet
        elif file_client.path_name.endswith(".parquet"):
            try:
                with io.BytesIO(file_data) as f:
                    pf = ParquetFile(f)
                    table = pf.read_row_groups([0], max_rows=1)
                    return table.num_rows > 0
            except Exception as e:
                print(f"❌ Parquet read failed: {e}")
                return False

        else:
            print(f"⚠️ Unsupported file extension: {file_client.path_name}")
            return False

    except Exception as e:
        print(f"❌ Error processing {file_client.path_name}: {e}")
        return False
